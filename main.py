import Adapter
import os
import dotenv
import time
import json
from pathlib import Path
from embedding import ChromaDBManager, build_azure_client_from_env

# --- 1. åˆå§‹åŒ–ç’°å¢ƒ ---
dotenv.load_dotenv()
azure_client = build_azure_client_from_env()
db_manager = ChromaDBManager(collection_name="stock_news", persist_dir=Path("db/chroma_db"))

AZURE_ENDPOINT = os.getenv("AZURE_ENDPOINT")
AZURE_API_VERSION = os.getenv("AZURE_API_VERSION")
AZURE_API_KEY = os.getenv("API_KEY")
client = Adapter.adapter(api_key=AZURE_API_KEY, endpoint=AZURE_ENDPOINT, api_version=AZURE_API_VERSION)

# ç¶­è­·å°è©±ç´€éŒ„
chat_history = []

def get_ai_response(user_input):
    global chat_history
    
    # --- æ­¥é©Ÿ Aï¼šåˆ¤æ–·æ˜¯å¦éœ€è¦ RAG (ç”±ç¨‹å¼é‚è¼¯æˆ–è¼•é‡åˆ¤æ–·) ---
    # å¦‚æœè¼¸å…¥åŒ…å«å•è™Ÿï¼Œæˆ–é•·åº¦é©ä¸­ï¼Œå‰‡å»è³‡æ–™åº«æ‰¾èƒŒæ™¯è³‡æ–™
    need_rag = "?" in user_input or "ï¼Ÿ" in user_input or len(user_input) < 100
    context = ""
    if need_rag:
        print("ğŸ” [ç³»çµ±] åµæ¸¬åˆ°ç–‘å•æ„åœ–ï¼Œæ­£åœ¨æª¢ç´¢è³‡æ–™åº«...")
        emb_res = azure_client.embeddings.create(input=[user_input], model="text-embedding-ada-002")
        q_emb = emb_res.data[0].embedding
        search_res = db_manager.query_by_embedding(query_embedding=q_emb, top_k=3)
        if search_res['documents'][0]:
            context = "\n".join(search_res['documents'][0])

    # --- æ­¥é©Ÿ Bï¼šå»ºæ§‹ Prompt (å¤šè¼ªå°è©± + ä¸Šä¸‹æ–‡) ---
    system_instruction = """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­è²¡ç¶“åŠ©æ‰‹ã€‚
    1. åƒè€ƒæä¾›çš„èƒŒæ™¯è³‡æ–™èˆ‡å°è©±æ­·å²å›ç­”å•é¡Œï¼Œä¸¦åŠ å…¥ä½ è‡ªå·±çš„è¦‹è§£ã€‚
    2. å¦‚æœä½¿ç”¨è€…è¼¸å…¥çš„æ˜¯å…·å‚™åƒ¹å€¼çš„è²¡ç¶“è³‡è¨Šï¼ˆå¦‚æ–°èã€æ•¸æ“šã€æ·±å…¥åˆ†æï¼‰ï¼Œè«‹åœ¨å›è¦†æœ€å¾Œæ¨™è¨» [SAVE_START] èˆ‡ [SAVE_END]ï¼Œä¸¦ä»¥ JSON æ ¼å¼æä¾›è©²å…§å®¹ã€‚
    JSON æ ¼å¼è¦æ±‚ï¼š{"title": "...", "content": "...", "date_publish": "YYYY-MM-DD"}
    """
    
    messages = [{"role": "system", "content": system_instruction}]
    # åŠ å…¥å°è©±æ­·å²
    messages.extend(chat_history[-6:]) # å–æœ€è¿‘ 3 è¼ªå°è©±
    
    # åŠ å…¥ç•¶å‰è¼¸å…¥èˆ‡ RAG ä¸Šä¸‹æ–‡
    current_prompt = f"ã€èƒŒæ™¯è³‡æ–™ã€‘ï¼š\n{context}\n\nã€ä½¿ç”¨è€…è¼¸å…¥ã€‘ï¼š{user_input}"
    messages.append({"role": "user", "content": current_prompt})

    # --- æ­¥é©Ÿ Cï¼šå‘¼å«æ¨¡å‹ ---
    body = {
        "model": "gpt-4o",
        "messages": messages,
        "temperature": 0.7,
    }
    
    response = client.create(body)
    full_content = response["choices"][0]["message"]["content"]
    
    # æ›´æ–°æ­·å²ç´€éŒ„
    chat_history.append({"role": "user", "content": user_input})
    chat_history.append({"role": "assistant", "content": full_content})
    
    return full_content

# --- 2. åŸ·è¡Œå¾ªç’° ---
print("ğŸ¤– è²¡ç¶“å°è©±æ©Ÿå™¨äººå·²ä¸Šç·š (è¼¸å…¥ 'exit' çµæŸå°è©±)")
while True:
    user_input = input("\nğŸ‘¤ ä½¿ç”¨è€…: ").strip()
    if user_input.lower() in ['exit', 'quit', 'bye']:
        break
    
    start_time = time.time()
    ai_reply = get_ai_response(user_input)
    
    # --- æ­¥é©Ÿ Dï¼šè™•ç†ã€Œå…¥åº«åƒ¹å€¼ã€åˆ¤æ–· ---
    if "[SAVE_START]" in ai_reply:
        print("\nâœ¨ [ç³»çµ±] åµæ¸¬åˆ°é«˜åƒ¹å€¼è³‡è¨Šï¼Œæº–å‚™å­˜å…¥ ChromaDB...")
        try:
            # æå– JSON å€å¡Š
            raw_json = ai_reply.split("[SAVE_START]")[1].split("[SAVE_END]")[0].strip()
            save_data = json.loads(raw_json)
            
            # å‘¼å«ä½ åŸæœ¬çš„ embedding.py é‚è¼¯é€²è¡Œ upsert
            # é€™è£¡ç°¡åŒ–æµç¨‹ï¼šç›´æ¥å°‡ content è½‰å‘é‡å­˜å…¥
            # (å»ºè­°åœ¨æ­¤è™•èª¿ç”¨ prepare_texts_with_splitter çš„é‚è¼¯)
            print(f"âœ… å·²æˆåŠŸè¨˜éŒ„ï¼š{save_data.get('title')}")
            
            # é¡¯ç¤ºçµ¦ä½¿ç”¨è€…çœ‹çš„å›è¦†å‰‡å»æ‰ JSON éƒ¨åˆ†
            clean_reply = ai_reply.split("[SAVE_START]")[0].strip()
        except Exception as e:
            print(f"âš ï¸ å…¥åº«æ ¼å¼è§£æå¤±æ•—: {e}")
            clean_reply = ai_reply
    else:
        clean_reply = ai_reply

    print(f"\nğŸ¤– åŠ©æ‰‹: {clean_reply}")
    print(f"(è€—æ™‚: {time.time() - start_time:.2f}s)")